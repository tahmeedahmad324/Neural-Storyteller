{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction: \nThis assignment is about building a multimodal deep learning model that generates natural language descriptions for images using a Sequence-to-Sequence (Seq2Seq) architecture.","metadata":{}},{"cell_type":"markdown","source":"# Environment Setup\nWe've created a notebook on **Kaggle**, and using the **Accelerator**: `GPU T4 x2 (Dual GPU)`\n\nFor the dataset, we're using the **Flickr30k** by *adityajn105*.\n\nLink: https://www.kaggle.com/datasets/adityajn105/flickr30k\n","metadata":{}},{"cell_type":"markdown","source":"# Part 1: Feature Extraction Pipeline","metadata":{}},{"cell_type":"code","source":"import os, pickle, torch, torch.nn as nn, torch.optim as optim\nimport re, pandas as pd, nltk, numpy as np, random, matplotlib.pyplot as plt, gradio as gr\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom nltk.tokenize import word_tokenize\nfrom nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:04:11.158660Z","iopub.execute_input":"2026-02-12T02:04:11.158894Z","iopub.status.idle":"2026-02-12T02:04:20.806311Z","shell.execute_reply.started":"2026-02-12T02:04:11.158867Z","shell.execute_reply":"2026-02-12T02:04:20.805631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_image_dir():\n    # Common Kaggle root\n    base_input = '/kaggle/input'\n    # Walk through the input directory to find where the images actually are\n    for root, dirs, files in os.walk(base_input):\n    # Look for the folder containing a high volume of jpg files\n        if len([f for f in files if f.endswith('.jpg')]) > 1000:\n            return root\n    return None\n\nIMAGE_DIR = find_image_dir()\nOUTPUT_FILE = 'flickr30k_features.pkl'\n\nif IMAGE_DIR:\n    print(f\" Found images at: {IMAGE_DIR}\")\nelse:\n    raise FileNotFoundError(\"Could not find the Flickr30k image directory. Please ensure the dataset is added to the notebook.\")\n\n\n# --- THE DATASET CLASS ---\nclass FlickrDataset(Dataset):\n    def __init__(self, img_dir, transform):\n        self.img_names = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg'))]\n        self.transform = transform\n        self.img_dir = img_dir\n    \n    def __len__(self):\n        return len(self.img_names)\n    def __getitem__(self, idx):\n        name = self.img_names[idx]\n        img_path = os.path.join(self.img_dir, name)\n        img = Image.open(img_path).convert('RGB')\n        return self.transform(img), name\n\n\n# --- REMAINDER OF THE PIPELINE (AS BEFORE) ---\nmodel = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\nmodel = nn.Sequential(*list(model.children())[:-1]) # Feature vector only\nmodel = nn.DataParallel(model).to(device)\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])\n\n\ndataset = FlickrDataset(IMAGE_DIR, transform)\nloader = DataLoader(dataset, batch_size=128, num_workers=4)\nfeatures_dict = {}\n\n\nwith torch.no_grad():\n for imgs, names in tqdm(loader, desc=\"Extracting Features\"):\n     feats = model(imgs.to(device)).view(imgs.size(0), -1)\n     for i, name in enumerate(names):\n         features_dict[name] = feats[i].cpu().numpy()\n\n\nwith open(OUTPUT_FILE, 'wb') as f:\n     pickle.dump(features_dict, f)\n\nprint(f\"Success! {len(features_dict)} images processed and saved to {OUTPUT_FILE}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:04:22.491086Z","iopub.execute_input":"2026-02-12T02:04:22.491399Z","iopub.status.idle":"2026-02-12T02:06:34.976821Z","shell.execute_reply.started":"2026-02-12T02:04:22.491370Z","shell.execute_reply":"2026-02-12T02:06:34.976098Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 2: Vocabulary & Text Pre-Processing\nSo here we're trying to convert text captions into numbers that the neural network can understand.\n\n1. **Load Data** â†’ Read captions from file into a dataframe (table)\n\n2. **Clean Text** â†’ Remove special characters, numbers, extra spaces, convert to lowercase\n\n3. **Tokenize** â†’ Split captions into individual words (tokens) using NLTK\n\n4. **Add Special Tokens** â†’ Add `<start>` and `<end>` markers to each caption\n\n5. **Build Vocabulary** â†’ \n   - Collect all words from all captions\n   - Keep only words appearing â‰¥5 times (removes rare/noisy words)\n   - Add special tokens: `<pad>`, `<start>`, `<end>`, `<unk>`\n\n6. **Create Mappings** â†’ \n   - word2idx: word â†’ number (e.g., \"dog\" â†’ 42)\n   - idx2word: number â†’ word (e.g., 42 â†’ \"dog\")\n\n7. **Convert to Numbers** â†’ Transform all captions from words to indices\n\n8. **Standardize Length** â†’\n   - Find MAX_LENGTH (95th percentile of caption lengths)\n   - Pad short captions with `<pad>` tokens\n   - Truncate long captions (keep `<end>` token)\n\n9. **Organize Data** â†’ Map each image to its 5 captions (as number sequences)\n\n10. **Save** â†’ Store vocabulary and image-caption mapping in pickle files\n\n**Output:** \n- `vocab.pkl` - Contains vocabulary and mappings\n- `image_to_captions.pkl` - Contains image â†’ captions dictionary\n\n**Result:** Raw text â†’ Standardized numerical sequences ready for neural network training! ðŸŽ¯","metadata":{}},{"cell_type":"code","source":"nltk.download('punkt')\n\n# --- LOAD DATA ---\ncaptions_file = '/kaggle/input/flickr30k/captions.txt'\ndataframe = pd.read_csv(captions_file, sep=',')\n\nprint(f\"Loaded {len(dataframe)} captions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:22.790877Z","iopub.execute_input":"2026-02-12T02:07:22.791473Z","iopub.status.idle":"2026-02-12T02:07:23.262130Z","shell.execute_reply.started":"2026-02-12T02:07:22.791441Z","shell.execute_reply":"2026-02-12T02:07:23.261359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CLEAN CAPTIONS ---\ndef clean_caption(caption):\n    caption = str(caption).lower()\n    caption = re.sub(r'[^a-z\\s]', '', caption)\n    caption = ' '.join(caption.split())\n    return caption\n\ndataframe['caption'] = dataframe['caption'].apply(clean_caption)\ndataframe['tokens'] = dataframe['caption'].apply(lambda x: x.split())\ndataframe['tokens'] = dataframe['tokens'].apply(lambda x: ['<start>'] + x + ['<end>'])\nprint(\"Example tokenized caption:\")\nprint(dataframe['tokens'].iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:26.918973Z","iopub.execute_input":"2026-02-12T02:07:26.919544Z","iopub.status.idle":"2026-02-12T02:07:28.114886Z","shell.execute_reply.started":"2026-02-12T02:07:26.919517Z","shell.execute_reply":"2026-02-12T02:07:28.114212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- BUILD VOCAB ---\n# Collect all tokens from all captions\nall_tokens = []\nfor token_list in dataframe['tokens']:\n    all_tokens.extend(token_list)\n\ntoken_counts = Counter(all_tokens)\n\n# Filter by minimum frequency\nMIN_FREQ = 5\nvocab = [token for token, count in token_counts.items() if count >= MIN_FREQ]\nspecial_tokens = ['<pad>', '<start>', '<end>', '<unk>']\nvocab = special_tokens + [v for v in vocab if v not in special_tokens]\nword2idx = {word: idx for idx, word in enumerate(vocab)}\nidx2word = {idx: word for idx, word in enumerate(vocab)}\n\nprint(f\"\\nVocabulary size: {len(vocab)}\")\nprint(f\"Most common tokens: {token_counts.most_common(20)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:31.479574Z","iopub.execute_input":"2026-02-12T02:07:31.479886Z","iopub.status.idle":"2026-02-12T02:07:31.772505Z","shell.execute_reply.started":"2026-02-12T02:07:31.479820Z","shell.execute_reply":"2026-02-12T02:07:31.771739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CONVERT TOKENS TO INDICES ---\ndef tokens_to_indices(tokens, word2idx):\n    indices = []\n    for token in tokens:\n        if token in word2idx:\n            indices.append(word2idx[token])\n        else:\n            indices.append(word2idx['<unk>'])\n    return indices\n\ndataframe['indices'] = dataframe['tokens'].apply(lambda x: tokens_to_indices(x, word2idx))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:34.760089Z","iopub.execute_input":"2026-02-12T02:07:34.760772Z","iopub.status.idle":"2026-02-12T02:07:35.360224Z","shell.execute_reply.started":"2026-02-12T02:07:34.760744Z","shell.execute_reply":"2026-02-12T02:07:35.359389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CHOOSE MAX LENGTH ---\n# Get length of each caption (in tokens)\ncaption_lengths = dataframe['tokens'].apply(len)\n\nMAX_LENGTH = int(np.percentile(caption_lengths, 95))\nprint(f\"\\nUsing MAX_LENGTH: {MAX_LENGTH}\")\n\n# Check how many captions will be truncated\nnum_truncated = (caption_lengths > MAX_LENGTH).sum()\nprint(f\"Captions to be truncated: {num_truncated} ({num_truncated/len(dataframe)*100:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:37.994887Z","iopub.execute_input":"2026-02-12T02:07:37.995186Z","iopub.status.idle":"2026-02-12T02:07:38.045627Z","shell.execute_reply.started":"2026-02-12T02:07:37.995160Z","shell.execute_reply":"2026-02-12T02:07:38.045027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- PADDING AND TRUNCATION FUNCTION ---\ndef pad_or_truncate(indices, max_length, pad_idx):\n    if len(indices) > max_length:\n        # Truncate (but keep <end> token)\n        return indices[:max_length-1] + [indices[-1]]  # Keep <end>\n    else:\n        # Pad with <pad> tokens\n        return indices + [pad_idx] * (max_length - len(indices))\n\n# Apply padding/truncation\nPAD_IDX = word2idx['<pad>']\ndataframe['padded_indices'] = dataframe['indices'].apply(\n    lambda x: pad_or_truncate(x, MAX_LENGTH, PAD_IDX)\n)\n\n# Verify all have same length\nassert all(len(x) == MAX_LENGTH for x in dataframe['padded_indices']), \"Not all sequences have MAX_LENGTH!\"\nprint(f\"\\nAll captions now have length {MAX_LENGTH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:47.340687Z","iopub.execute_input":"2026-02-12T02:07:47.341088Z","iopub.status.idle":"2026-02-12T02:07:47.778482Z","shell.execute_reply.started":"2026-02-12T02:07:47.341060Z","shell.execute_reply":"2026-02-12T02:07:47.777798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- UPDATE IMAGE-TO-CAPTIONS MAPPING ---\nimage_to_captions = {}\nfor idx, row in dataframe.iterrows():\n    img_name = row['image']\n    caption_indices = row['padded_indices']\n    \n    if img_name not in image_to_captions:\n        image_to_captions[img_name] = []\n    \n    image_to_captions[img_name].append(caption_indices)\n\n# --- SAVE WITH MAX_LENGTH ---\nwith open('vocab.pkl', 'wb') as frame:\n    pickle.dump({\n        'word2idx': word2idx,\n        'idx2word': idx2word,\n        'vocab': vocab,\n        'max_length': MAX_LENGTH,\n        'pad_idx': PAD_IDX\n    }, frame)\n\nwith open('image_to_captions.pkl', 'wb') as frame:\n    pickle.dump(image_to_captions, frame)\n\nprint(\"\\n --- Preprocessing complete --- \")\nprint(f\"Vocabulary size: {len(vocab)}\")\nprint(f\"Max caption length: {MAX_LENGTH}\")\nprint(f\"Total images: {len(image_to_captions)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:52.083069Z","iopub.execute_input":"2026-02-12T02:07:52.083357Z","iopub.status.idle":"2026-02-12T02:07:57.753176Z","shell.execute_reply.started":"2026-02-12T02:07:52.083333Z","shell.execute_reply":"2026-02-12T02:07:57.752482Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 3: The Seq2Seq Architecture ","metadata":{}},{"cell_type":"code","source":"# Load vocabulary\nwith open('vocab.pkl', 'rb') as f:\n    vocab_data = pickle.load(f)\n    word2idx = vocab_data['word2idx']\n    idx2word = vocab_data['idx2word']\n    vocab = vocab_data['vocab']\n    MAX_LENGTH = vocab_data['max_length']\n    PAD_IDX = vocab_data['pad_idx']\n\nwith open('flickr30k_features.pkl', 'rb') as f:\n    image_features = pickle.load(f)\n\nwith open('image_to_captions.pkl', 'rb') as f:\n    image_to_captions = pickle.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"VOCAB_SIZE = len(vocab)\nEMBED_SIZE = 256\nHIDDEN_SIZE = 512\nFEATURE_SIZE = 2048\nNUM_LAYERS = 1\n\nBATCH_SIZE = 64\nLEARNING_RATE = 0.001\nNUM_EPOCHS = 10\n\nprint(f\"Vocab Size: {VOCAB_SIZE}\")\nprint(f\"Embedding Size: {EMBED_SIZE}\")\nprint(f\"Hidden Size: {HIDDEN_SIZE}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, feature_size=2048, hidden_size=512):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        \n        self.fc = nn.Linear(feature_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, features):\n        hidden = self.fc(features)\n        hidden = self.relu(hidden)\n        hidden = self.dropout(hidden)\n        return hidden\n\nencoder = Encoder(FEATURE_SIZE, HIDDEN_SIZE).to(device)\nprint(\"Encoder side initialized\")\nprint(encoder)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n        super(Decoder, self).__init__()\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_IDX)\n        self.lstm = nn.LSTM(\n            embed_size, \n            hidden_size, \n            num_layers, \n            batch_first=True,\n            dropout=0.5 if num_layers > 1 else 0\n        )\n        self.fc = nn.Linear(hidden_size, vocab_size)\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, captions, hidden):\n        embeddings = self.embedding(captions)\n        embeddings = self.dropout(embeddings)\n        lstm_out, hidden = self.lstm(embeddings, hidden)\n        outputs = self.fc(lstm_out)\n        return outputs, hidden\n\n\ndecoder = Decoder(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(device)\nprint(\"Decoder created successfully!\")\nprint(decoder)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Seq2SeqModel(nn.Module):\n    def __init__(self, encoder, decoder, hidden_size, num_layers):\n        super(Seq2SeqModel, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n    \n    def forward(self, image_features, captions):\n        batch_size = image_features.size(0)\n        encoder_hidden = self.encoder(image_features)\n        h_0 = encoder_hidden.unsqueeze(0).repeat(self.num_layers, 1, 1)\n        c_0 = torch.zeros_like(h_0)\n        outputs, _ = self.decoder(captions, (h_0, c_0))\n        return outputs\n    \n    def generate(self, image_features, max_length, start_token, end_token):\n        self.eval()\n        with torch.no_grad():\n            encoder_hidden = self.encoder(image_features)\n            h = encoder_hidden.unsqueeze(0).repeat(self.num_layers, 1, 1)\n            c = torch.zeros_like(h)\n\n            current_word = torch.tensor([[start_token]]).to(device)\n            generated_caption = [start_token]\n            \n            for _ in range(max_length):\n                output, (h, c) = self.decoder(current_word, (h, c))\n                predicted_word_idx = output.argmax(dim=2).item()\n                generated_caption.append(predicted_word_idx)\n                if predicted_word_idx == end_token:\n                    break\n\n                current_word = torch.tensor([[predicted_word_idx]]).to(device)\n        \n        return generated_caption\n\nmodel = Seq2SeqModel(encoder, decoder, HIDDEN_SIZE, NUM_LAYERS).to(device)\nprint(\"\\nModel creation complete\")\nprint(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 4: Training & Inference","metadata":{}},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self, image_features, image_to_captions, split='train', train_ratio=0.8):\n        self.image_features = image_features\n        self.image_to_captions = image_to_captions\n        all_images = list(image_to_captions.keys())\n        random.shuffle(all_images)\n        split_idx = int(len(all_images) * train_ratio)\n        if split == 'train':\n            self.images = all_images[:split_idx]\n        else:\n            self.images = all_images[split_idx:]\n        self.data = []\n        for img in self.images:\n            if img in self.image_features:\n                for caption in self.image_to_captions[img]:\n                    self.data.append((img, caption))\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        img_name, caption = self.data[idx]\n        features = torch.tensor(self.image_features[img_name], dtype=torch.float32)\n        caption_tensor = torch.tensor(caption[:-1], dtype=torch.long)\n        target_tensor = torch.tensor(caption[1:], dtype=torch.long)\n        return features, caption_tensor, target_tensor\n\ntrain_dataset = FlickrDataset(image_features, image_to_captions, split='train', train_ratio=0.8)\nval_dataset = FlickrDataset(image_features, image_to_captions, split='val', train_ratio=0.8)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\nprint(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Val samples: {len(val_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nprint(\"Loss function: CrossEntropyLoss\")\nprint(\"Optimizer: Adam\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    for features, captions, targets in tqdm(loader, desc=\"Training\"):\n        features = features.to(device)\n        captions = captions.to(device)\n        targets = targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(features, captions)\n        loss = criterion(outputs.view(-1, VOCAB_SIZE), targets.view(-1))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate_epoch(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for features, captions, targets in tqdm(loader, desc=\"Validation\"):\n            features = features.to(device)\n            captions = captions.to(device)\n            targets = targets.to(device)\n            outputs = model(features, captions)\n            loss = criterion(outputs.view(-1, VOCAB_SIZE), targets.view(-1))\n            total_loss += loss.item()\n    return total_loss / len(loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses = []\nval_losses = []\nbest_val_loss = float('inf')\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss = validate_epoch(model, val_loader, criterion, device)\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'best_model.pth')\n        print(\"Model saved!\")\nprint(\"\\nTraining complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Train Loss', marker='o')\nplt.plot(val_losses, label='Val Loss', marker='s')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def greedy_search(model, image_features, max_length, start_token, end_token):\n    model.eval()\n    with torch.no_grad():\n        encoder_hidden = model.encoder(image_features)\n        h = encoder_hidden.unsqueeze(0).repeat(model.num_layers, 1, 1)\n        c = torch.zeros_like(h)\n        current_word = torch.tensor([[start_token]]).to(device)\n        generated = [start_token]\n        for _ in range(max_length):\n            output, (h, c) = model.decoder(current_word, (h, c))\n            predicted = output.argmax(dim=2).item()\n            generated.append(predicted)\n            if predicted == end_token:\n                break\n            current_word = torch.tensor([[predicted]]).to(device)\n    return generated","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def beam_search(model, image_features, max_length, start_token, end_token, beam_width=3):\n    model.eval()\n    with torch.no_grad():\n        encoder_hidden = model.encoder(image_features)\n        h = encoder_hidden.unsqueeze(0).repeat(model.num_layers, 1, 1)\n        c = torch.zeros_like(h)\n        sequences = [[[start_token], 0.0, h, c]]\n        for _ in range(max_length):\n            all_candidates = []\n            for seq, score, h_state, c_state in sequences:\n                if seq[-1] == end_token:\n                    all_candidates.append([seq, score, h_state, c_state])\n                    continue\n                current_word = torch.tensor([[seq[-1]]]).to(device)\n                output, (new_h, new_c) = model.decoder(current_word, (h_state, c_state))\n                probs = torch.log_softmax(output[0, 0], dim=0)\n                top_probs, top_indices = probs.topk(beam_width)\n                for i in range(beam_width):\n                    candidate = [seq + [top_indices[i].item()], score + top_probs[i].item(), new_h, new_c]\n                    all_candidates.append(candidate)\n            ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n            sequences = ordered[:beam_width]\n        return sequences[0][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\nimage_dir = '/kaggle/input/flickr30k/flickr30k_images/flickr30k_images'\ntest_images = random.sample(val_dataset.images, 5)\nSTART_IDX = word2idx['<start>']\nEND_IDX = word2idx['<end>']\nfor img_name in test_images:\n    img_path = os.path.join(image_dir, img_name)\n    img = Image.open(img_path)\n    plt.figure(figsize=(8, 6))\n    plt.imshow(img)\n    plt.axis('off')\n    features = torch.tensor(image_features[img_name]).unsqueeze(0).to(device)\n    greedy_caption = greedy_search(model, features, MAX_LENGTH, START_IDX, END_IDX)\n    beam_caption = beam_search(model, features, MAX_LENGTH, START_IDX, END_IDX, beam_width=3)\n    greedy_words = ' '.join([idx2word[idx] for idx in greedy_caption if idx not in [PAD_IDX, START_IDX, END_IDX]])\n    beam_words = ' '.join([idx2word[idx] for idx in beam_caption if idx not in [PAD_IDX, START_IDX, END_IDX]])\n    gt_caption = image_to_captions[img_name][0]\n    gt_words = ' '.join([idx2word[idx] for idx in gt_caption if idx not in [PAD_IDX, START_IDX, END_IDX]])\n    plt.title(f\"GT: {gt_words}\\nGreedy: {greedy_words}\\nBeam: {beam_words}\", fontsize=10)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"references = []\nhypotheses_greedy = []\nhypotheses_beam = []\nfor img_name in tqdm(val_dataset.images[:500], desc=\"Evaluating\"):\n    features = torch.tensor(image_features[img_name]).unsqueeze(0).to(device)\n    greedy_caption = greedy_search(model, features, MAX_LENGTH, START_IDX, END_IDX)\n    beam_caption = beam_search(model, features, MAX_LENGTH, START_IDX, END_IDX, beam_width=3)\n    greedy_words = [idx2word[idx] for idx in greedy_caption if idx not in [PAD_IDX, START_IDX, END_IDX]]\n    beam_words = [idx2word[idx] for idx in beam_caption if idx not in [PAD_IDX, START_IDX, END_IDX]]\n    ref_captions = [[idx2word[idx] for idx in cap if idx not in [PAD_IDX, START_IDX, END_IDX]] for cap in image_to_captions[img_name]]\n    references.append(ref_captions)\n    hypotheses_greedy.append(greedy_words)\n    hypotheses_beam.append(beam_words)\nbleu_greedy = corpus_bleu(references, hypotheses_greedy)\nbleu_beam = corpus_bleu(references, hypotheses_beam)\nprint(f\"BLEU-4 (Greedy): {bleu_greedy:.4f}\")\nprint(f\"BLEU-4 (Beam): {bleu_beam:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\ndef calculate_metrics(references, hypotheses, vocab_size):\n    all_ref_tokens = []\n    all_hyp_tokens = []\n    for refs, hyp in zip(references, hypotheses):\n        ref_tokens = set()\n        for ref in refs:\n            ref_tokens.update(ref)\n        all_ref_tokens.extend([1 if i in ref_tokens else 0 for i in range(vocab_size)])\n        hyp_set = set(hyp)\n        all_hyp_tokens.extend([1 if i in hyp_set else 0 for i in range(vocab_size)])\n    precision = precision_score(all_ref_tokens, all_hyp_tokens, average='binary', zero_division=0)\n    recall = recall_score(all_ref_tokens, all_hyp_tokens, average='binary', zero_division=0)\n    f1 = f1_score(all_ref_tokens, all_hyp_tokens, average='binary', zero_division=0)\n    return precision, recall, f1\nprint(\"Calculating metrics...\")\nprec, rec, f1 = calculate_metrics(references, hypotheses_beam, VOCAB_SIZE)\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall: {rec:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save({\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'vocab': vocab,\n    'word2idx': word2idx,\n    'idx2word': idx2word,\n    'max_length': MAX_LENGTH\n}, 'final_model.pth')\nprint(\"Final model saved!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gradio App","metadata":{}},{"cell_type":"code","source":"model.eval()\ndef generate_caption(image):\n    img_array = np.array(image.resize((224, 224)))\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n    ])\n    img_tensor = transform(Image.fromarray(img_array)).unsqueeze(0).to(device)\n    resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n    resnet = nn.Sequential(*list(resnet.children())[:-1]).to(device)\n    resnet.eval()\n    with torch.no_grad():\n        features = resnet(img_tensor).view(1, -1)\n    caption_indices = beam_search(model, features, MAX_LENGTH, word2idx['<start>'], word2idx['<end>'], beam_width=3)\n    caption = ' '.join([idx2word[idx] for idx in caption_indices if idx not in [PAD_IDX, word2idx['<start>'], word2idx['<end>']]])\n    return caption\ndemo = gr.Interface(\n    fn=generate_caption,\n    inputs=gr.Image(type=\"pil\"),\n    outputs=gr.Textbox(label=\"Generated Caption\"),\n    title=\"Image Captioning Model\",\n    description=\"Upload an image to generate a caption\"\n)\ndemo.launch(share=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}