{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction: \nThis assignment is about building a multimodal deep learning model that generates natural language descriptions for images using a Sequence-to-Sequence (Seq2Seq) architecture.","metadata":{}},{"cell_type":"markdown","source":"# Environment Setup\nWe've created a notebook on kaggle, and using the Accelerator: GPU T4 x2 (Dual GPU)\nFor the dataset, we're using the Flickr30k by adityajn105.\nLink: https://www.kaggle.com/datasets/adityajn105/flickr30k\n","metadata":{}},{"cell_type":"markdown","source":"# Part 1: Feature Extraction Pipeline","metadata":{}},{"cell_type":"code","source":"import os, pickle, torch, torch.nn as nn, re, pandas as pd, nltk\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom nltk.tokenize import word_tokenize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T07:15:18.334960Z","iopub.execute_input":"2026-02-11T07:15:18.335539Z","iopub.status.idle":"2026-02-11T07:15:26.091803Z","shell.execute_reply.started":"2026-02-11T07:15:18.335510Z","shell.execute_reply":"2026-02-11T07:15:26.091006Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def find_image_dir():\n    # Common Kaggle root\n    base_input = '/kaggle/input'\n    # Walk through the input directory to find where the images actually are\n    for root, dirs, files in os.walk(base_input):\n    # Look for the folder containing a high volume of jpg files\n        if len([f for f in files if f.endswith('.jpg')]) > 1000:\n            return root\n    return None\n    \nIMAGE_DIR = find_image_dir()\nOUTPUT_FILE = 'flickr30k_features.pkl'\n\nif IMAGE_DIR:\n    print(f\" Found images at: {IMAGE_DIR}\")\nelse:\n    raise FileNotFoundError(\"Could not find the Flickr30k image directory. Please ensure the dataset is added to the notebook.\")\n\n\n# --- THE DATASET CLASS ---\nclass FlickrDataset(Dataset):\n    def __init__(self, img_dir, transform):\n        self.img_names = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg'))]\n        self.transform = transform\n        self.img_dir = img_dir\n    \n    def __len__(self):\n        return len(self.img_names)\n    def __getitem__(self, idx):\n        name = self.img_names[idx]\n        img_path = os.path.join(self.img_dir, name)\n        img = Image.open(img_path).convert('RGB')\n        return self.transform(img), name\n\n\n# --- REMAINDER OF THE PIPELINE (AS BEFORE) ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\nmodel = nn.Sequential(*list(model.children())[:-1]) # Feature vector only\nmodel = nn.DataParallel(model).to(device)\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])\n\n\ndataset = FlickrDataset(IMAGE_DIR, transform)\nloader = DataLoader(dataset, batch_size=128, num_workers=4)\nfeatures_dict = {}\n\n\nwith torch.no_grad():\n for imgs, names in tqdm(loader, desc=\"Extracting Features\"):\n     feats = model(imgs.to(device)).view(imgs.size(0), -1)\n     for i, name in enumerate(names):\n         features_dict[name] = feats[i].cpu().numpy()\n\n\nwith open(OUTPUT_FILE, 'wb') as f:\n     pickle.dump(features_dict, f)\n\nprint(f\"Success! {len(features_dict)} images processed and saved to {OUTPUT_FILE}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-08T16:55:19.333158Z","iopub.execute_input":"2026-02-08T16:55:19.334140Z","iopub.status.idle":"2026-02-08T16:57:33.939376Z","shell.execute_reply.started":"2026-02-08T16:55:19.334108Z","shell.execute_reply":"2026-02-08T16:57:33.938534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 2: Vocabulary & Text Pre-Processing","metadata":{}},{"cell_type":"code","source":"nltk.download('punkt')\n\n# --- LOAD DATA ---\ncaptions_file = '/kaggle/input/flickr30k/captions.txt'\ndataframe = pd.read_csv(captions_file, sep=',')\n\nprint(f\"Loaded {len(dataframe)} captions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T07:20:29.457306Z","iopub.execute_input":"2026-02-11T07:20:29.458455Z","iopub.status.idle":"2026-02-11T07:20:29.818158Z","shell.execute_reply.started":"2026-02-11T07:20:29.458413Z","shell.execute_reply":"2026-02-11T07:20:29.817541Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Loaded 158915 captions\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- CLEAN CAPTIONS ---\ndef clean_caption(caption):\n    caption = str(caption).lower()\n    caption = re.sub(r'[^a-z\\s]', '', caption)\n    caption = ' '.join(caption.split())\n    return caption\n\ndataframe['caption'] = dataframe['caption'].apply(clean_caption)\ndataframe['tokens'] = dataframe['tokens'].apply(lambda x: ['<start>'] + x + ['<end>'])\nprint(\"Example tokenized caption:\")\nprint(dataframe['tokens'].iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T08:13:25.496179Z","iopub.execute_input":"2026-02-11T08:13:25.496906Z","iopub.status.idle":"2026-02-11T08:13:26.074478Z","shell.execute_reply.started":"2026-02-11T08:13:25.496877Z","shell.execute_reply":"2026-02-11T08:13:26.073713Z"}},"outputs":[{"name":"stdout","text":"Example tokenized caption:\n['<start>', '<start>', '<start>', 'two', 'young', 'guys', 'with', 'shaggy', 'hair', 'look', 'at', 'their', 'hands', 'while', 'hanging', 'out', 'in', 'the', 'yard', '<end>', '<end>', '<end>']\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# --- BUILD VOCAB ---\n# Collect all tokens from all captions\nall_tokens = []\nfor token_list in dataframe['tokens']:\n    all_tokens.extend(token_list)\n\ntoken_counts = Counter(all_tokens)\n\n# Filter by minimum frequency\nMIN_FREQ = 5\nvocab = [token for token, count in token_counts.items() if count >= MIN_FREQ]\nspecial_tokens = ['<pad>', '<start>', '<end>', '<unk>']\nvocab = special_tokens + [v for v in vocab if v not in special_tokens]\nword2idx = {word: idx for idx, word in enumerate(vocab)}\nidx2word = {idx: word for idx, word in enumerate(vocab)}\n\nprint(f\"\\nVocabulary size: {len(vocab)}\")\nprint(f\"Most common tokens: {token_counts.most_common(20)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T08:13:32.991583Z","iopub.execute_input":"2026-02-11T08:13:32.992296Z","iopub.status.idle":"2026-02-11T08:13:33.292689Z","shell.execute_reply.started":"2026-02-11T08:13:32.992266Z","shell.execute_reply":"2026-02-11T08:13:33.292071Z"}},"outputs":[{"name":"stdout","text":"\nVocabulary size: 7528\nMost common tokens: [('<start>', 476745), ('<end>', 476745), ('a', 271704), ('in', 83466), ('the', 62978), ('on', 45669), ('and', 44263), ('man', 42598), ('is', 41116), ('of', 38776), ('with', 36207), ('woman', 22211), ('two', 21641), ('are', 20196), ('to', 17607), ('people', 17337), ('at', 16261), ('an', 15882), ('wearing', 15709), ('young', 13218)]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# --- CONVERT TOKENS TO INDICES ---\ndef tokens_to_indices(tokens, word2idx):\n    indices = []\n    for token in tokens:\n        if token in word2idx:\n            indices.append(word2idx[token])\n        else:\n            indices.append(word2idx['<unk>'])\n    return indices\n\ndataframe['indices'] = dataframe['tokens'].apply(lambda x: tokens_to_indices(x, word2idx))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T07:54:16.832160Z","iopub.execute_input":"2026-02-11T07:54:16.832926Z","iopub.status.idle":"2026-02-11T07:54:17.461722Z","shell.execute_reply.started":"2026-02-11T07:54:16.832892Z","shell.execute_reply":"2026-02-11T07:54:17.461121Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T07:57:01.800376Z","iopub.execute_input":"2026-02-11T07:57:01.800670Z","iopub.status.idle":"2026-02-11T07:57:01.804302Z","shell.execute_reply.started":"2026-02-11T07:57:01.800645Z","shell.execute_reply":"2026-02-11T07:57:01.803534Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# --- CHOOSE MAX LENGTH ---\n# Get length of each caption (in tokens)\ncaption_lengths = dataframe['tokens'].apply(len)\n\nMAX_LENGTH = int(np.percentile(caption_lengths, 95))\nprint(f\"\\nUsing MAX_LENGTH: {MAX_LENGTH}\")\n\n# Check how many captions will be truncated\nnum_truncated = (caption_lengths > MAX_LENGTH).sum()\nprint(f\"Captions to be truncated: {num_truncated} ({num_truncated/len(dataframe)*100:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T07:58:31.906931Z","iopub.execute_input":"2026-02-11T07:58:31.907510Z","iopub.status.idle":"2026-02-11T07:58:31.957745Z","shell.execute_reply.started":"2026-02-11T07:58:31.907480Z","shell.execute_reply":"2026-02-11T07:58:31.957070Z"}},"outputs":[{"name":"stdout","text":"\nUsing MAX_LENGTH: 26\nCaptions to be truncated: 6556 (4.13%)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# --- PADDING AND TRUNCATION FUNCTION ---\ndef pad_or_truncate(indices, max_length, pad_idx):\n    if len(indices) > max_length:\n        # Truncate (but keep <end> token)\n        return indices[:max_length-1] + [indices[-1]]  # Keep <end>\n    else:\n        # Pad with <pad> tokens\n        return indices + [pad_idx] * (max_length - len(indices))\n\n# Apply padding/truncation\nPAD_IDX = word2idx['<pad>']\ndataframe['padded_indices'] = dataframe['indices'].apply(\n    lambda x: pad_or_truncate(x, MAX_LENGTH, PAD_IDX)\n)\n\n# Verify all have same length\nassert all(len(x) == MAX_LENGTH for x in dataframe['padded_indices']), \"Not all sequences have MAX_LENGTH!\"\nprint(f\"\\nAll captions now have length {MAX_LENGTH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T08:09:47.536614Z","iopub.execute_input":"2026-02-11T08:09:47.537491Z","iopub.status.idle":"2026-02-11T08:09:48.043858Z","shell.execute_reply.started":"2026-02-11T08:09:47.537460Z","shell.execute_reply":"2026-02-11T08:09:48.043205Z"}},"outputs":[{"name":"stdout","text":"\nAll captions now have length 26\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# === UPDATE IMAGE-TO-CAPTIONS MAPPING ===\nimage_to_captions = {}\nfor idx, row in df.iterrows():\n    img_name = row['image']\n    caption_indices = row['padded_indices']\n    \n    if img_name not in image_to_captions:\n        image_to_captions[img_name] = []\n    \n    image_to_captions[img_name].append(caption_indices)\n\n# === SAVE WITH MAX_LENGTH ===\nwith open('vocab.pkl', 'wb') as f:\n    pickle.dump({\n        'word2idx': word2idx,\n        'idx2word': idx2word,\n        'vocab': vocab,\n        'max_length': MAX_LENGTH,  # Save this too!\n        'pad_idx': PAD_IDX\n    }, f)\n\nwith open('image_to_captions.pkl', 'wb') as f:\n    pickle.dump(image_to_captions, f)\n\nprint(\"\\n✓ All preprocessing complete!\")\nprint(f\"✓ Vocabulary size: {len(vocab)}\")\nprint(f\"✓ Max caption length: {MAX_LENGTH}\")\nprint(f\"✓ Total images: {len(image_to_captions)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === CREATE IMAGE-TO-CAPTIONS MAPPING ===\nimage_to_captions = {}\nfor idx, row in df.iterrows():\n    img_name = row['image']\n    caption_indices = row['indices']\n    \n    if img_name not in image_to_captions:\n        image_to_captions[img_name] = []\n    \n    image_to_captions[img_name].append(caption_indices)\n\n# === SAVE EVERYTHING ===\nwith open('vocab.pkl', 'wb') as f:\n    pickle.dump({\n        'word2idx': word2idx,\n        'idx2word': idx2word,\n        'vocab': vocab\n    }, f)\n\nwith open('image_to_captions.pkl', 'wb') as f:\n    pickle.dump(image_to_captions, f)\n\nprint(\"\\n✓ Vocabulary saved to vocab.pkl\")\nprint(\"✓ Image-caption mapping saved to image_to_captions.pkl\")\nprint(f\"✓ Total images: {len(image_to_captions)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T06:58:59.272360Z","iopub.execute_input":"2026-02-11T06:58:59.273562Z","iopub.status.idle":"2026-02-11T06:58:59.538035Z","shell.execute_reply.started":"2026-02-11T06:58:59.273532Z","shell.execute_reply":"2026-02-11T06:58:59.537097Z"}},"outputs":[{"name":"stdout","text":"Loaded 158915 captions\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/3230974318.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caption'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caption'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_caption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caption'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataFrame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caption'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'<start> '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' <end>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n","\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3230974318.py\u001b[0m in \u001b[0;36mclean_caption\u001b[0;34m(caption)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# --- CLEAN CAPTIONS ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^a-z\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"],"ename":"AttributeError","evalue":"'float' object has no attribute 'lower'","output_type":"error"}],"execution_count":3}]}