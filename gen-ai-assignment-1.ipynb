{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction: \nThis assignment is about building a multimodal deep learning model that generates natural language descriptions for images using a Sequence-to-Sequence (Seq2Seq) architecture.","metadata":{}},{"cell_type":"markdown","source":"# Environment Setup\nWe've created a notebook on **Kaggle**, and using the **Accelerator**: `GPU T4 x2 (Dual GPU)`\n\nFor the dataset, we're using the **Flickr30k** by *adityajn105*.\n\nLink: https://www.kaggle.com/datasets/adityajn105/flickr30k\n","metadata":{}},{"cell_type":"markdown","source":"# Part 1: Feature Extraction Pipeline","metadata":{}},{"cell_type":"code","source":"import os, pickle, torch, torch.nn as nn, re, pandas as pd, nltk, numpy as np\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom nltk.tokenize import word_tokenize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:04:11.158660Z","iopub.execute_input":"2026-02-12T02:04:11.158894Z","iopub.status.idle":"2026-02-12T02:04:20.806311Z","shell.execute_reply.started":"2026-02-12T02:04:11.158867Z","shell.execute_reply":"2026-02-12T02:04:20.805631Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def find_image_dir():\n    # Common Kaggle root\n    base_input = '/kaggle/input'\n    # Walk through the input directory to find where the images actually are\n    for root, dirs, files in os.walk(base_input):\n    # Look for the folder containing a high volume of jpg files\n        if len([f for f in files if f.endswith('.jpg')]) > 1000:\n            return root\n    return None\n    \nIMAGE_DIR = find_image_dir()\nOUTPUT_FILE = 'flickr30k_features.pkl'\n\nif IMAGE_DIR:\n    print(f\" Found images at: {IMAGE_DIR}\")\nelse:\n    raise FileNotFoundError(\"Could not find the Flickr30k image directory. Please ensure the dataset is added to the notebook.\")\n\n\n# --- THE DATASET CLASS ---\nclass FlickrDataset(Dataset):\n    def __init__(self, img_dir, transform):\n        self.img_names = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg'))]\n        self.transform = transform\n        self.img_dir = img_dir\n    \n    def __len__(self):\n        return len(self.img_names)\n    def __getitem__(self, idx):\n        name = self.img_names[idx]\n        img_path = os.path.join(self.img_dir, name)\n        img = Image.open(img_path).convert('RGB')\n        return self.transform(img), name\n\n\n# --- REMAINDER OF THE PIPELINE (AS BEFORE) ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\nmodel = nn.Sequential(*list(model.children())[:-1]) # Feature vector only\nmodel = nn.DataParallel(model).to(device)\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])\n\n\ndataset = FlickrDataset(IMAGE_DIR, transform)\nloader = DataLoader(dataset, batch_size=128, num_workers=4)\nfeatures_dict = {}\n\n\nwith torch.no_grad():\n for imgs, names in tqdm(loader, desc=\"Extracting Features\"):\n     feats = model(imgs.to(device)).view(imgs.size(0), -1)\n     for i, name in enumerate(names):\n         features_dict[name] = feats[i].cpu().numpy()\n\n\nwith open(OUTPUT_FILE, 'wb') as f:\n     pickle.dump(features_dict, f)\n\nprint(f\"Success! {len(features_dict)} images processed and saved to {OUTPUT_FILE}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:04:22.491086Z","iopub.execute_input":"2026-02-12T02:04:22.491399Z","iopub.status.idle":"2026-02-12T02:06:34.976821Z","shell.execute_reply.started":"2026-02-12T02:04:22.491370Z","shell.execute_reply":"2026-02-12T02:06:34.976098Z"}},"outputs":[{"name":"stdout","text":" Found images at: /kaggle/input/flickr30k/Images\nDownloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 176MB/s] \nExtracting Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [01:53<00:00,  2.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Success! 31783 images processed and saved to flickr30k_features.pkl\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Part 2: Vocabulary & Text Pre-Processing\nSo here we're trying to convert text captions into numbers that the neural network can understand.\n\n1. **Load Data** â†’ Read captions from file into a dataframe (table)\n\n2. **Clean Text** â†’ Remove special characters, numbers, extra spaces, convert to lowercase\n\n3. **Tokenize** â†’ Split captions into individual words (tokens) using NLTK\n\n4. **Add Special Tokens** â†’ Add `<start>` and `<end>` markers to each caption\n\n5. **Build Vocabulary** â†’ \n   - Collect all words from all captions\n   - Keep only words appearing â‰¥5 times (removes rare/noisy words)\n   - Add special tokens: `<pad>`, `<start>`, `<end>`, `<unk>`\n\n6. **Create Mappings** â†’ \n   - word2idx: word â†’ number (e.g., \"dog\" â†’ 42)\n   - idx2word: number â†’ word (e.g., 42 â†’ \"dog\")\n\n7. **Convert to Numbers** â†’ Transform all captions from words to indices\n\n8. **Standardize Length** â†’\n   - Find MAX_LENGTH (95th percentile of caption lengths)\n   - Pad short captions with `<pad>` tokens\n   - Truncate long captions (keep `<end>` token)\n\n9. **Organize Data** â†’ Map each image to its 5 captions (as number sequences)\n\n10. **Save** â†’ Store vocabulary and image-caption mapping in pickle files\n\n**Output:** \n- `vocab.pkl` - Contains vocabulary and mappings\n- `image_to_captions.pkl` - Contains image â†’ captions dictionary\n\n**Result:** Raw text â†’ Standardized numerical sequences ready for neural network training! ðŸŽ¯","metadata":{}},{"cell_type":"code","source":"nltk.download('punkt')\n\n# --- LOAD DATA ---\ncaptions_file = '/kaggle/input/flickr30k/captions.txt'\ndataframe = pd.read_csv(captions_file, sep=',')\n\nprint(f\"Loaded {len(dataframe)} captions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:22.790877Z","iopub.execute_input":"2026-02-12T02:07:22.791473Z","iopub.status.idle":"2026-02-12T02:07:23.262130Z","shell.execute_reply.started":"2026-02-12T02:07:22.791441Z","shell.execute_reply":"2026-02-12T02:07:23.261359Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Loaded 158915 captions\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- CLEAN CAPTIONS ---\ndef clean_caption(caption):\n    caption = str(caption).lower()\n    caption = re.sub(r'[^a-z\\s]', '', caption)\n    caption = ' '.join(caption.split())\n    return caption\n\ndataframe['caption'] = dataframe['caption'].apply(clean_caption)\ndataframe['tokens'] = dataframe['caption'].apply(lambda x: x.split())\ndataframe['tokens'] = dataframe['tokens'].apply(lambda x: ['<start>'] + x + ['<end>'])\nprint(\"Example tokenized caption:\")\nprint(dataframe['tokens'].iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:26.918973Z","iopub.execute_input":"2026-02-12T02:07:26.919544Z","iopub.status.idle":"2026-02-12T02:07:28.114886Z","shell.execute_reply.started":"2026-02-12T02:07:26.919517Z","shell.execute_reply":"2026-02-12T02:07:28.114212Z"}},"outputs":[{"name":"stdout","text":"Example tokenized caption:\n['<start>', 'two', 'young', 'guys', 'with', 'shaggy', 'hair', 'look', 'at', 'their', 'hands', 'while', 'hanging', 'out', 'in', 'the', 'yard', '<end>']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- BUILD VOCAB ---\n# Collect all tokens from all captions\nall_tokens = []\nfor token_list in dataframe['tokens']:\n    all_tokens.extend(token_list)\n\ntoken_counts = Counter(all_tokens)\n\n# Filter by minimum frequency\nMIN_FREQ = 5\nvocab = [token for token, count in token_counts.items() if count >= MIN_FREQ]\nspecial_tokens = ['<pad>', '<start>', '<end>', '<unk>']\nvocab = special_tokens + [v for v in vocab if v not in special_tokens]\nword2idx = {word: idx for idx, word in enumerate(vocab)}\nidx2word = {idx: word for idx, word in enumerate(vocab)}\n\nprint(f\"\\nVocabulary size: {len(vocab)}\")\nprint(f\"Most common tokens: {token_counts.most_common(20)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:31.479574Z","iopub.execute_input":"2026-02-12T02:07:31.479886Z","iopub.status.idle":"2026-02-12T02:07:31.772505Z","shell.execute_reply.started":"2026-02-12T02:07:31.479820Z","shell.execute_reply":"2026-02-12T02:07:31.771739Z"}},"outputs":[{"name":"stdout","text":"\nVocabulary size: 7689\nMost common tokens: [('a', 271705), ('<start>', 158915), ('<end>', 158915), ('in', 83466), ('the', 62978), ('on', 45669), ('and', 44263), ('man', 42598), ('is', 41117), ('of', 38776), ('with', 36207), ('woman', 22211), ('two', 21642), ('are', 20196), ('to', 17607), ('people', 17337), ('at', 16259), ('an', 15883), ('wearing', 15709), ('young', 13218)]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- CONVERT TOKENS TO INDICES ---\ndef tokens_to_indices(tokens, word2idx):\n    indices = []\n    for token in tokens:\n        if token in word2idx:\n            indices.append(word2idx[token])\n        else:\n            indices.append(word2idx['<unk>'])\n    return indices\n\ndataframe['indices'] = dataframe['tokens'].apply(lambda x: tokens_to_indices(x, word2idx))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:34.760089Z","iopub.execute_input":"2026-02-12T02:07:34.760772Z","iopub.status.idle":"2026-02-12T02:07:35.360224Z","shell.execute_reply.started":"2026-02-12T02:07:34.760744Z","shell.execute_reply":"2026-02-12T02:07:35.359389Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# --- CHOOSE MAX LENGTH ---\n# Get length of each caption (in tokens)\ncaption_lengths = dataframe['tokens'].apply(len)\n\nMAX_LENGTH = int(np.percentile(caption_lengths, 95))\nprint(f\"\\nUsing MAX_LENGTH: {MAX_LENGTH}\")\n\n# Check how many captions will be truncated\nnum_truncated = (caption_lengths > MAX_LENGTH).sum()\nprint(f\"Captions to be truncated: {num_truncated} ({num_truncated/len(dataframe)*100:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:37.994887Z","iopub.execute_input":"2026-02-12T02:07:37.995186Z","iopub.status.idle":"2026-02-12T02:07:38.045627Z","shell.execute_reply.started":"2026-02-12T02:07:37.995160Z","shell.execute_reply":"2026-02-12T02:07:38.045027Z"}},"outputs":[{"name":"stdout","text":"\nUsing MAX_LENGTH: 24\nCaptions to be truncated: 6779 (4.27%)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# --- PADDING AND TRUNCATION FUNCTION ---\ndef pad_or_truncate(indices, max_length, pad_idx):\n    if len(indices) > max_length:\n        # Truncate (but keep <end> token)\n        return indices[:max_length-1] + [indices[-1]]  # Keep <end>\n    else:\n        # Pad with <pad> tokens\n        return indices + [pad_idx] * (max_length - len(indices))\n\n# Apply padding/truncation\nPAD_IDX = word2idx['<pad>']\ndataframe['padded_indices'] = dataframe['indices'].apply(\n    lambda x: pad_or_truncate(x, MAX_LENGTH, PAD_IDX)\n)\n\n# Verify all have same length\nassert all(len(x) == MAX_LENGTH for x in dataframe['padded_indices']), \"Not all sequences have MAX_LENGTH!\"\nprint(f\"\\nAll captions now have length {MAX_LENGTH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:47.340687Z","iopub.execute_input":"2026-02-12T02:07:47.341088Z","iopub.status.idle":"2026-02-12T02:07:47.778482Z","shell.execute_reply.started":"2026-02-12T02:07:47.341060Z","shell.execute_reply":"2026-02-12T02:07:47.777798Z"}},"outputs":[{"name":"stdout","text":"\nAll captions now have length 24\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# --- UPDATE IMAGE-TO-CAPTIONS MAPPING ---\nimage_to_captions = {}\nfor idx, row in dataframe.iterrows():\n    img_name = row['image']\n    caption_indices = row['padded_indices']\n    \n    if img_name not in image_to_captions:\n        image_to_captions[img_name] = []\n    \n    image_to_captions[img_name].append(caption_indices)\n\n# --- SAVE WITH MAX_LENGTH ---\nwith open('vocab.pkl', 'wb') as frame:\n    pickle.dump({\n        'word2idx': word2idx,\n        'idx2word': idx2word,\n        'vocab': vocab,\n        'max_length': MAX_LENGTH,  # Save this too!\n        'pad_idx': PAD_IDX\n    }, frame)\n\nwith open('image_to_captions.pkl', 'wb') as frame:\n    pickle.dump(image_to_captions, frame)\n\nprint(\"\\nAll preprocessing complete\")\nprint(f\"Vocabulary size: {len(vocab)}\")\nprint(f\"Max caption length: {MAX_LENGTH}\")\nprint(f\"Total images: {len(image_to_captions)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T02:07:52.083069Z","iopub.execute_input":"2026-02-12T02:07:52.083357Z","iopub.status.idle":"2026-02-12T02:07:57.753176Z","shell.execute_reply.started":"2026-02-12T02:07:52.083333Z","shell.execute_reply":"2026-02-12T02:07:57.752482Z"}},"outputs":[{"name":"stdout","text":"\nAll preprocessing complete!\nVocabulary size: 7689\nMax caption length: 24\nTotal images: 31783\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T06:58:59.272360Z","iopub.execute_input":"2026-02-11T06:58:59.273562Z","iopub.status.idle":"2026-02-11T06:58:59.538035Z","shell.execute_reply.started":"2026-02-11T06:58:59.273532Z","shell.execute_reply":"2026-02-11T06:58:59.537097Z"}},"outputs":[],"execution_count":null}]}